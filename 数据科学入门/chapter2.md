深度学习讲义

 Table of Contents 深度学习讲义线性代数标量、向量、矩阵、张量转置和广播矩阵的线性运算矩阵和向量相乘线性方程组矩阵元素对应乘积单位矩阵和逆矩阵线性相关和生成子空间范数特殊类型的矩阵和向量特征分解奇异值分解Moore-Penrose伪逆迹运算行列式主成分分析概率信息论数值计算机器学习基础理论学习算法容量、过拟合和欠拟合超参数和验证集估计、偏差和方差最大似然估计贝叶斯统计监督学习算法无监督学习算法随机梯度下降构建机器学习算法深度学习基础理论深度学习发展的必要性深度前馈网络深度学习中的正则化深度学习中的优化卷积神经网络循环神经网络深度增强学习生成式对抗网络 


\huge 应知应会的数学 \\
\large  线性代数 \\
 \begin{flushleft}
 \end{flushleft}


线性代数

标量、向量、矩阵、张量

- 标量：标量就是一个数 
- 向量：向量就是一列数
  
\begin{bmatrix}x_{1}\\x_{2}\\\vdots\\x_{n}\end{bmatrix}

  
- 矩阵：矩阵就是二维数组
  
\begin{bmatrix}a && b\\c && d\\\end{bmatrix}

- 张量：张量就是多维数组

转置和广播

- 转置：转置就是矩阵以对角线为轴做镜像操作
- 广播：将向量与矩阵中的每一行或者每一列做线性运算

矩阵的线性运算

- 矩阵和矩阵的加减法满足对应位置元素做线性运算
- 标量和矩阵相乘或者标量和矩阵相加 <=> 矩阵的每个元素与标量相加或者相乘

矩阵和向量相乘

矩阵A与矩阵B相乘，必须满足矩阵A的列数和矩阵B的行数相等。


C = AB\\
C_{i,j}=\sum_{k}A_{i,k}B_{k,j}


矩阵乘积满足分配率和结合率，但是不满足交换律。

矩阵乘积的转置性质：


(AB)^T=B^TA^T


线性方程组


A_{1,1}x_{1} + A_{1,2}x_{2}+\cdots A_{1,n}x_{n} = b_{1}\\
A_{2,1}x_{1} + A_{2,2}x_{2}+\cdots A_{2,n}x_{n} = b_{2}\\
\cdots\\
A_{m,1}x_{1} + A_{m,2}x_{2}+\cdots A_{m,n}x_{n} = b_{1}




矩阵元素对应乘积

矩阵元素对应乘积存在，被称为Hadamard乘积，记为 


A\odot B


单位矩阵和逆矩阵

任意向量和单位矩阵相乘，都不会改变，将保持n维向量不变的单位矩阵记作


I_{n}\\


形式上，


I_{n}\mathbb{R}^{n \times n},\\
\forall x\in\mathbb{R}^n,I_{n}x=x.


单位矩阵的主对角线上的元素都是1，其他所有位置的元素都是0。


\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0  & 0 & 1\end{bmatrix}


矩阵A的逆矩阵记作


A^{-1}


满足条件：


A^{-1}A = I_{n}


线性方程的解


Ax=b\\
A^{-1}Ax=A^{-1}b\\
I_{n}x=A^{-1}b\\
x=A^{-1}b


线性相关和生成子空间

若线性方程的解为x，y，则


z=\alpha x+(1-\alpha)x


也是方程的解。


向量x中的每个元素


范数

特殊类型的矩阵和向量

特征分解

奇异值分解

Moore-Penrose伪逆

迹运算

行列式

主成分分析

概率

信息论

数值计算

机器学习基础理论

学习算法

容量、过拟合和欠拟合

超参数和验证集

估计、偏差和方差

最大似然估计

贝叶斯统计

监督学习算法

无监督学习算法

随机梯度下降

构建机器学习算法

深度学习基础理论

深度学习发展的必要性

深度前馈网络

深度学习中的正则化

深度学习中的优化

卷积神经网络

循环神经网络

深度增强学习

生成式对抗网络



 
